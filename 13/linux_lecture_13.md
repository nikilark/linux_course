# Розділ 13: Віртуалізація

Слово "віртуальний" може бути неоднозначним у комп'ютерних системах. Воно використовується головним чином для позначення посередника, який перетворює складний або фрагментований нижній рівень на спрощений інтерфейс, який може бути використаний декількома користувачами.

Розгляньте приклад, який ми вже бачили, віртуальна пам'ять, яка дозволяє декільком процесам отримати доступ до великого банку пам'яті так, ніби кожен мав свій власний ізольований банк пам'яті.

![Віртуальна пам'ять](images/virtual_memory.png)

Це визначення все ще трохи складне, тому може бути краще пояснити типову мету віртуалізації: створення ізольованих середовищ, щоб ви могли запустити декілька систем без конфліктів.

## Віртуальні машини

Віртуальні машини базуються на тому ж концепті, що й віртуальна пам'ять, за винятком того, що _все_ обладнання машини є віртуальним, а не лише пам'ять.

За цією моделлю ви створюєте цілком нову машину (процесор, пам'ять, інтерфейси вводу-виводу та інше) за допомогою програмного забезпечення і запускаєте цілковито нову операційну систему в ній — включаючи ядро. Цей тип віртуальної машини більш конкретно називається _системною віртуальною машиною_, і він існує вже десятиліттями.

Наприклад, IBM-мейнфрейми традиційно використовують системні віртуальні машини для створення багатокористувацького середовища; в свою чергу, користувачі отримують свою власну віртуальну машину, на якій запущена CMS, проста однокористувацька операційна система.

Ви можете побудувати віртуальну машину цілком на програмному забезпеченні (зазвичай це називається _емулятором_) або використовувати справжнє обладнання наскільки це можливо, як це робиться в віртуальній пам'яті.

![Віртуальна машина](images/vm.png)

### Гіпервізори

Керування однією або декількома віртуальними машинами на комп'ютері здійснюється за допомогою програмного забезпечення, яке називається _гіпервізором_ або _віртуальним монітором машин (VMM)_, яке працює подібно до того, як операційна система керує процесами.

Існують два типи гіпервізорів, і спосіб використання віртуальної машини залежить від типу.

Для більшості користувачів _гіпервізор другого типу_ є найбільш знайомим, оскільки він працює на звичайній операційній системі, такій як Linux. Наприклад, VirtualBox є гіпервізором другого типу, і ви можете запустити його на своїй системі без значних змін.

З іншого боку, _гіпервізор першого типу_ більше схожий на власне операційну систему (особливо ядро), побудовану спеціально для швидкого та ефективного запуску віртуальних машин. Цей тип гіпервізора іноді може використовувати звичайну додаткову систему, таку як Linux, для допомоги в управлінні завданнями.

Навіть якщо ви ніколи не запускали його на власному обладнанні, ви взаємодієте з гіпервізорами першого типу весь час. Усі хмарні обчислення працюють як віртуальні машини під управлінням гіпервізорів першого типу, таких як Xen. Коли ви отримуєте доступ до веб-сайту, ви ймовірно працюєте з програмним забезпеченням, що працює віртуальній машині.

_Створення екземпляру операційної системи на хмарному сервісі, такому як AWS, означає створення віртуальної машини на гіпервізорі першого типу._

Взагалі, віртуальну машину з її операційною системою називають _гостем_. _Хост_ — це те, що запускає гіпервізор. Для гіпервізорів другого типу хостом є просто ваша операційна система. Для гіпервізорів першого типу хостом є сам гіпервізор, можливо, в поєднанні зі спеціалізованою допоміжною системою.

### Обладнання у віртуальній машині

У теорії гіпервізору повинно бути просто надати обладнані інтерфейси для гостьової системи.

Наприклад, щоб мати віртуальний пристрій диска, ви можете створити великий файл десь на хості та надати доступ як до диска зі стандартною емуляцією введення-виведення. Цей підхід реалізує віртуальне обладнання; однак він неефективний.

Більшість відмінностей, які ви можете спостерігати між реальним та віртуальним обладнанням, є результатом прокидування (_bridging_), що дозволяє гостям отримувати більш прямий доступ до ресурсів хоста. Обхід віртуального обладнання між хостом та гостем відомий як _пара-віртуалізація_.

Мережеві інтерфейси та блочні пристрої найбільш схильні до такого підходу; наприклад, існує пристрій _/dev/xvd_ на хмарній системі є віртуальним диском Xen, який використовує драйвер ядра Linux для прямого спілкування з гіпервізором.

#### Режими ЦП віртуальної машини

Конкретні назви цих режимів варіюються залежно від процесора (наприклад, процесори x86 використовують систему, що називається _кільця привілеїв_), але ідея завжди одна й та ж. У режимі ядра процесор може робити майже все; у режимі користувача деякі інструкції заборонені, а доступ до пам'яті обмежений.

Перші віртуальні машини для архітектури x86 працювали в режимі користувача. Це створило проблему, оскільки ядро, що виконується всередині віртуальної машини, хоче бути в режимі ядра. Для компенсації цього гіпервізор може виявляти та реагувати на перехоплювати (_trapping_) будь-які обмежені інструкції, що надходять від віртуальної машини.

За допомогою невеликої роботи гіпервізор емулює обмежені інструкції, що дозволяє віртуальним машинам працювати в режимі ядра на архітектурі, не призначеній для цього. Оскільки більшість інструкцій, які виконує ядро, не обмежені, вони виконуються нормально, і вплив на продуктивність є досить мінімальним.

Незабаром після введення цього типу гіпервізора виробники процесорів зрозуміли, що існує попит на процесори, які можуть допомагати гіпервізору, усуваючи потребу в емуляції інструкцій. Intel і AMD випустили ці набори функцій як VT-x і AMD-V відповідно, і більшість гіпервізорів тепер їх підтримують. У деяких випадках вони навіть обов'язкові.

### Використання віртуальних машин

У світі Linux використання віртуальних машин часто підпадає під одну з наступних категорій:

1. **Тестування та випробування** Існує багато сценаріїв використання віртуальних машин, коли вам потрібно спробувати щось за межами персонального або корпоративного операційного середовища. Наприклад, коли ви розробляєте корпоративне програмне забезпечення, важливо протестувати програмне забезпечення на машині, що відокремлена від розробника. Ще одним використанням є експерименти з новим програмним забезпеченням, таким як новий дистрибутив, в безпечному і "одноразовому" середовищі. Віртуальні машини дозволяють це зробити, не купуючи нове обладнання.
2. **Сумісність програм** Коли вам потрібно запустити щось під операційною системою, відмінною від персональної, віртуальні машини необхідні.
3. **Сервери та хмарні сервіси** Як було зазначено раніше, всі хмарні сервіси побудовані на технології віртуальних машин. Якщо вам потрібно запустити Інтернет-сервер, наприклад, веб-сервер, найшвидший спосіб це зробити - заплатити хмарному постачальнику за екземпляр віртуальної машини. Хмарні постачальники також пропонують спеціалізовані сервери, такі як бази даних, які являють собою просто налаштовані набори програмного забезпечення, що працюють на віртуальних машинах.

### Недоліки віртуальних машин

* _Може бути незручно та часово затратно встановлювати та/або налаштовувати систему та програму_. Інструменти, такі як Ansible, можуть автоматизувати цей процес, але все ще потрібно багато часу, щоб побудувати систему з нуля.
* _Навіть при правильній конфігурації віртуальні машини запускаються та перезавантажуються відносно повільно_.
* _Ви повинні підтримувати повну систему Linux, оновлюючи та забезпечуючи безпеку кожної віртуальної машини_.
* _Ваше програмному забезпеченню може мати конфлікти зі стандартним набором програм на віртуальній машині_.
* _Відокремлення ваших сервісів на різні віртуальні машини може бути неефективним та дорогим_.

## Віртуалізація на основі середовища виконання

Причина застосування цього типу віртуалізації полягає в тому, що кілька програм на одній системі можуть використовувати одну й ту ж мову програмування, що може призводити до потенційних конфліктів.

Наприклад, Python використовується в кількох місцях у типовому дистрибутиві та може включати багато додаткових пакетів. Якщо ви хочете використовувати версію Python системи, ви можете отримати проблеми, якщо захочете іншу версію одного з додаткових пакетів.

Давайте розглянемо, як функція віртуального середовища Python створює версію Python лише з пакетами, які вам потрібні. Запуск віртуального середовища полягає у створенні нового каталогу для цього середовища, наприклад, так:

    python3 -m venv test-venv

Тепер подивіться всередину нового каталогу _test-venv_. Ви побачите кілька схожих на системні каталоги, таких як _bin_, _include_ та _lib_. Щоб активувати віртуальне середовище, вам потрібно включити (`source` або `.`) сценарій `test-venv/bin/activate`:

    . test-env/bin/activate

Причина виконання цієї команди полягає в тому, що активація в основному полягає в установці змінних середовища, що неможливо зробити, запустивши виконуваний файл.

На цьому етапі, коли ви запускаєте Python, ви отримуєте версію `python` в каталозі _test-venv/bin_ (який сам по собі є лише символічним посиланням), і змінна середовища `VIRTUAL_ENV` встановлюється на базовий каталог середовища.

Ви можете запустити `deactivate`, щоб вийти з віртуального середовища.

![Віртуальне середовище](images/venv.jpg)

З цією змінною середовища ви отримуєте нову, порожню бібліотеку пакетів у _test-venv/lib_, і все нове, що ви встановлюєте під час перебування у середовищі, буде встановлено туди замість основної бібліотеки системи.

## Контейнери

Віртуальні машини чудово ізолюють всю операційну систему та набір запущених додатків, але іноді вам потрібна більш легка альтернатива для установки бар'єрів навколо служб сервера, особливо коли ви не дуже довіряєте одній з них. Технологія контейнерів зараз є популярним способом вирішення цієї проблеми.

Одним з методів ізоляції служб є використання системного виклику `chroot()` для зміни кореневого каталогу на щось інше, ніж фактичний кореневий каталог системи. Цей тип ізоляції іноді називають _тюремною ямою chroot_ (_chroot jail_), оскільки процеси не можуть (зазвичай) вийти з неї.

Іншим типом обмеження є лімітування ресурсів (`rlimit`) ядра, які обмежують час ЦП, який процес може споживати, або розмір його файлів.

_Контейнер_ можна визначити як обмежене середовище виконання для набору процесів так, що ці процеси не можуть торкатися нічого на системі поза цим середовищем. Загалом це називається _віртуалізацією на рівні операційної системи_.

![Container](images/docker.webp)

Важливо пам'ятати, що машина, на якій працює один або кілька контейнерів, все ще має лише одне базове ядро Linux. Однак процеси всередині контейнера можуть використовувати середовище користувача з іншого дистрибутиву Linux.

Обмеження в контейнерах побудовані за допомогою кількох функцій ядра. Деякі з важливих аспектів процесів, які працюють у контейнері:

* Вони мають свої власні `cgroups`.
* Вони мають свої власні пристрої та файлову систему.
* Вони не можуть бачити або взаємодіяти з іншими процесами на системі.
* Вони мають свої власні мережеві інтерфейси.

### LXC

Термін _LXC_ іноді використовується для визначення набіру функцій ядра, які роблять контейнери можливими, але більшість людей використовують його конкретно для бібліотеки та пакетів, які містять кілька утиліт для створення та маніпулювання контейнерами Linux.

На відміну від Docker, LXC вимагає досить багато ручного налаштування. Наприклад, вам потрібно створити власний мережевий інтерфейс контейнера, і вам потрібно перекинути ідентифікатори користувачів.

Спочатку LXC мав бути повноцінною системою Linux всередині container—init і все. Після встановлення спеціальної версії дистрибутива ви могли встановити все необхідне для того, що ви запускали всередині контейнера.

Тому ви можете вважати LXC більш гнучким для пристосування до різних потреб.

### Kubernetes

Контейнери стали популярними для багатьох видів веб-серверів, оскільки ви можете запустити кілька контейнерів з одного образу на кількох машинах, забезпечуючи відмінну надійність.

На жаль, менеджети це може бути важко. Вам потрібно виконувати такі завдання, як:

* Відстежувати, які машини можуть запускати контейнери.
* Запускати, контролювати та перезапускати контейнери на цих машинах.
* Налаштовувати запуск контейнера.
* Налаштовувати мережу контейнера за потребою.
* Завантажувати нові версії образів контейнера та оновлювати всі запущені контейнери безпомилково.

_Kubernetes_ від Google з'явився, щоб вирішити це. Можливо, одним з найбільших факторів, що спричинили його популярність, є його можливість запускати образи контейнерів Docker.

![Kubernetes](images/kubernetes.png)

Kubernetes має дві основні сторони, як і будь-яка клієнт-серверна програма. Сервер включає машини, доступні для запуску контейнерів, а клієнт це в основному набір утиліт командного рядка, які запускають і маніпулюють наборами контейнерів.

Файли конфігурації для контейнерів (і груп, які вони формують) можуть бути обширними, і ви швидко зрозумієте, що більшість роботи на клієнтському боці полягає в створенні відповідної конфігурації.

Якщо ви не хочете самостійно налаштовувати сервери, використовуйте інструмент _Minikube_ для встановлення віртуальної машини, на якій запускається кластер Kubernetes, на вашому хості.

### Недоліки контейнерів

* _Контейнери можуть витрачати забагато дискового простору_.
* _Вам все ще потрібно думати про інші системні ресурси, такі як час ЦП_.
* _Вам може доводитись думати по-іншому про те, де ви зберігаєте свої дані_. У системах контейнерів, таких як Docker, що використовують накладені файлові системи, зміни, внесені до файлової системи під час роботи, видаляються після завершення процесів.
* _Більшість інструментів та моделей роботи з контейнерами орієнтовані на веб-сервери_.
* _Недбале створення контейнерів може призвести до хаосу, проблем конфігурації та несправностей_.
* _Версіонування може бути проблематичним_. Одна із стандартних практик полягає в тому, щоб використовувати певний тег версії базового контейнера.
* _Не завжди можна довіряти автору образа_. Це стосується особливо образів, попередньо побудованих за допомогою Docker. Це відрізняється від LXC, де вас закликають будувати свої власні образи до певної міри.

Просто пам'ятайте, що контейнери не вирішать кожну проблему. Наприклад, якщо вашому додатку потрібно багато часу для запуску на звичайній системі (після завантаження), він так само повільно запуститься і в контейнері.

### Робота з Docker

Для роботи з Docker спочатку потрібно створити _образ_ (_image_), який включає файлову систему та кілька інших важливих функцій для запуску контейнера. Ваші образи майже завжди будуть базуватися на завантажених з репозитарію в Інтернеті вже побудованих образах.

>**ПРИМІТКА**: Легко переплутати образи і контейнери. Ви можете думати про образ як про файлову систему контейнера; процеси не запускаються в образі, але вони запускаються в контейнерах. Це не зовсім точно (зокрема, коли ви змінюєте файли в контейнері Docker, ви не робите зміни в образі), але достатньо близько.

Встановіть Docker на свою систему, створіть десь новий каталог, перейдіть в цей каталог та створіть файл з назвою _Dockerfile_, що містить наступні рядки:

    FROM alpine:latest
    RUN apk add bash
    CMD ["/bin/bash"]

У цій конфігурації використовується легкий дистрибутив Alpine. Єдине, що ми змінюємо, - це додавання оболонки bash, що ми робимо не тільки для покращення можливості інтерактивного використання, але й для створення унікального образу та перегляду, як працює ця процедура.

Можливо (і це зазвичай відбувається), ви будете використовувати публічні образи та не робитимете до них жодних змін. У цьому випадку вам не потрібен Dockerfile.

Побудуйте образ за допомогою наступної команди, яка зчитує файл Dockerfile в поточному каталозі та надає ідентифікатор `hlw_test` для образу:

    docker build -t hlw_test .

>**ПРИМІТКА**: Можливо, вам доведеться додати себе до групи _docker_ на вашій системі, щоб мати змогу виконувати команди Docker від імені звичайного користувача.

Будьте готові до великої кількості повідомлень. Перше завдання - отримати останню версію контейнера дистрибутиву Alpine з реєстру Docker:

    Sending build context to Docker daemon  2.048kB
    Step 1/3 : FROM alpine:latest
    latest: Pulling from library/alpine
    cbdbe7a5bc2a: Pull complete 
    Digest: sha256:9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4b9a54
    Status: Downloaded newer image for alpine:latest
     ---> f70734b6a266

На цьому етапі Docker створив новий образ з ідентифікатором `f70734b6a266` для базового образу дистрибутиву Alpine. Ви можете посилатися на цей конкретний образ пізніше, але вам, ймовірно, не знадобиться, оскільки він не є кінцевим. Далі Docker буде будувати наступні образи на його основі. Образи, які не призначені для кінцевого використиання, називаються _проміжними_ (_intermediate image_).

Наступна частина нашої конфігурації - встановлення пакету оболонки bash в Alpine.

    Step 2/3 : RUN apk add bash
     ---> Running in 4f0fb4632b31
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main/x86_64/APKINDEX.tar.gz
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community/x86_64/APKINDEX.tar.gz
    (1/4) Installing ncurses-terminfo-base (6.1_p20200118-r4)
    (2/4) Installing ncurses-libs (6.1_p20200118-r4)
    (3/4) Installing readline (8.0.1-r0)
    (4/4) Installing bash (5.0.11-r1)
    Executing bash-5.0.11-r1.post-install
    Executing busybox-1.31.1-r9.trigger
    OK: 8 MiB in 18 packages
    Removing intermediate container 4f0fb4632b31
     ---> 12ef4043c80a

Те, що не так очевидно, - це _як_ це відбувається. Ключовим є рядок, що говорить `Running in 4f0fb4632b31`. Ви ще не просили про створення контейнеру, але Docker вже його налаштував із допомогою проміжного образу Alpine з попереднього кроку.

Після налаштування (тимчасового) контейнера з ідентифікатором `4f0fb4632b31`, Docker виконав команду `apk` всередині цього контейнера для встановлення bash, а потім зберіг результати змін у файловій системі в новому проміжному образі з ідентифікатором `12ef4043c80a`. Зверніть увагу, що Docker також видаляє контейнер після завершення.

Нарешті, Docker робить останні зміни, необхідні для запуску оболонки bash при запуску контейнера з нового образу:

    Step 3/3 : CMD ["/bin/bash"]
     ---> Running in fb082e6a0728
    Removing intermediate container fb082e6a0728
     ---> 1b64f94e5a54
    Successfully built 1b64f94e5a54
    Successfully tagged hlw_test:latest

>**ПРИМІТКА**: Будь-які дії, виконані за допомогою команди RUN в Dockerfile, відбуваються під час побудови образу, а не після, коли ви запускаєте контейнер. Команда CMD призначена для виконання контейнером; саме тому вона відбувається в кінці.

У цьому прикладі ви отримали кінцевий образ з ідентифікатором `1b64f94e5a54`, але через те, що ви позначили його (окремий крок), ви також можете посилатися на нього як на `hlw_test` або `hlw_test:latest`.

Виконайте `docker images`, щоб перевірити, що ваш образ та образ Alpine присутні:

    $ docker images
    REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
    hlw_test            latest              1b64f94e5a54        1 minute ago        9.19MB
    alpine              latest              f70734b6a266        3 weeks ago         5.61MB

#### Команди для побудови образів

* `ADD` - Додає файли та каталоги з вашої системи до образу.
* `ARG` - Встановлює змінні, які можна використовувати під час збірки.
* `CMD` - Вказує команду, яка виконується під час запуску контейнера.
* `COPY` - Копіює файли та каталоги з вашої системи до образу.
* `ENTRYPOINT` - Вказує команду, яка виконується під час запуску контейнера.
* `ENV` - Встановлює змінні середовища.
* `EXPOSE` - Вказує порти, які контейнер відкриває для зовнішнього доступу.
* `FROM` - Вказує базовий образ для побудови.
* `HEALTHCHECK` - Вказує команду для перевірки стану контейнера.
* `LABEL` - Встановлює мітки для образу.
* `MAINTAINER` - Вказує автора образу.
* `ONBUILD` - Вказує команди, які виконуються під час побудови образу, який базується на цьому образі.
* `RUN` - Виконує команду під час побудови образу.
* `SHELL` - Вказує оболонку за замовчуванням для виконання команд.
* `STOPSIGNAL` - Вказує сигнал, який використовується для зупинки контейнера.
* `USER` - Вказує користувача, який виконує команди.
* `VOLUME` - Вказує каталоги, які можуть бути монтувані зовнішніми контейнерами.
* `WORKDIR` - Вказує робочий каталог для команд.

#### Запуск контейнерів Docker

Тепер ви готові запустити контейнер.

    docker run -it hlw_test

Ви повинні отримати запрошення оболонки bash, де ви можете виконувати команди в контейнері. Ця оболонка запускається від імені root користувача.

>**ПРИМІТКА**: Якщо ви забудете опції `-it` (інтерактивний, підключення терміналу), ви не отримаєте запрошення оболонки, і контейнер завершиться майже миттєво. Ці опції нечасто використовуються при звичайному використанні Docker (особливо `-t`).

Ви швидко помітите, що хоча здебільшого все виглядає як типова система Linux, дещо все ж таки відрізняється. Наприклад, якщо ви надрукуєте повний список процесів, ви отримаєте всього два записи:

    # ps aux
    PID   USER     TIME  COMMAND
        1 root      0:00 /bin/bash
        6 root      0:00 ps aux

Якимось чином, у контейнері оболонка має ідентифікатор процесу 1 (пам'ятайте, на звичайній системі це _init_), і нічого крім виконання процесу, який ви запустили.

На цьому етапі важливо пам'ятати, що ці процеси - це просто ті, які ви можете бачити на вашій звичайній (хостовій) системі. Якщо ви відкриєте інше вікно оболонки на вашій хостовій системі, ви можете знайти процес контейнера в списку:

    root     20189  0.2  0.0   2408  2104 pts/0    Ss+  08:36   0:00 /bin/bash

Це одна з функцій ядра, яка використовуються для підтримки роботи контейнерів: простори імен (_namespaces_) ядра Linux, створені для ідентифікаторів процесів. Процес може створити цілком новий набір ідентифікаторів процесів для себе та своїх дочірніх процесів, починаючи з PID 1, і тоді вони можуть бачити тільки ті процеси, які є в цьому просторі імен.

#### Накладені (_overlay_) файлові системи

Далі дослідіть файлову систему у вашому контейнері. Переглянувши як змонтована коренева файлова система, ви побачите, що вона відрізняється від того, що ви бачите на вашій хостовій системі:

    overlay on / type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/
    C3D66CQYRP4SCXWFFY6HHF6X5Z:/var/lib/docker/overlay2/l/K4BLIOMNRROX3SS5GFPB
    7SFISL:/var/lib/docker/overlay2/l/2MKIOXW5SUB2YDOUBNH4G4Y7KF1,upperdir=/
    var/lib/docker/overlay2/d064be6692c0c6ff4a45ba9a7a02f70e2cf5810a15bcb2b728b00
    dc5b7d0888c/diff,workdir=/var/lib/docker/overlay2/d064be6692c0c6ff4a45ba9a7a02
    f70e2cf5810a15bcb2b728b00dc5b7d0888c/work)

Це _накладена файлова система_ (_overlay filesystem_), функція ядра, яка дозволяє вам створювати файлову систему, комбінуючи існуючі каталоги як шари, зі змінами, збереженими в одному місці. Якщо ви подивитеся на вашу хостову систему, ви побачите це (та матимете доступ до складових каталогів), і ви також знайдете, де Docker створив початкову точку маунту.

#### Мережа

Перед тим, як щось запускати, Docker створює новий мережевий інтерфейс (зазвичай _docker0_) на хостовій системі для приватної мережі. Ця мережа призначена для зв'язку між хост-машинами та їхніми контейнерами.

Для зв'язку із зовнішніми хостами мережа Docker на вашому хості налаштовує NAT (_Network Address Translation_).

Вона включає фізичний шар з інтерфейсами, а також інтернет-шар підмережі Docker та NAT, який зв'язує цю підмережу з рештою хост-машини та її зовнішніми підключеннями.

Docker контейнер також може бути приєднаним до мережі хоста, у цьому випадку він матиме той самий мережевий інтерфейс, що і хост. Для цього вам потрібно запустити контейнер з опцією `--network host`.

    docker run -it --network host hlw_test

Ви також можете вказати порти, які будуть переадресовані з хоста до контейнера. Наприклад, щоб переадресувати порт 2222 на хості на порт 22 контейнера, ви можете запустити наступну команду:

    docker run -it -p 2222:22 hlw_test

#### Певні операції в Docker

Docker позначає контейнер як "запущений" (_running_) доти, доки в ньому працює процес. Ви можете побачити запущені контейнери за допомогою `docker ps`:

    $ docker ps
    CONTAINER ID   IMAGE       COMMAND       CREATED       STATUS      PORTS    NAMES
    bda6204cecf7   hlw_test    "/bin/bash"   8 hours ago   Up 8 hours           boring_lovelace
    8a48d6e85efe   hlw_test    "/bin/bash"   20 hours ago  Up 20 hours          awesome_elion

Як тільки всі процеси контейнера завершаться, Docker переведе їх у стан виходу, але він все ще зберігає контейнер (якщо ви не запустите його з опціїєю `--rm`). Це включає зміни, внесені до файлової системи. Ви легко можете отримати доступ до файлової системи за допомогою `docker export`.

    docker export bda6204cecf7 > my_container.tar

`docker ps` не показує завершені контейнери за замовчуванням; ви повинні використовувати опцію `-a`, щоб побачити їх.

Дуже легко накопичити велику кількість завершених контейнерів, і якщо програма, що працює в контейнері, створює багато даних, ви можете вичерпати дисковий простір без явної причини. Використовуйте `docker rm`, щоб видалити завершений контейнер.

Це також стосується старих образів. Якщо ви виконаєте `docker images`, ви можете побачити всі образи на вашій системі. Ось приклад виводу попередньої версії тестового образу без тегу:

    $ docker images
    REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
    hlw_test            latest              1b64f94e5a54        43 hours ago        9.19MB
    <none>              <none>              d0461f65b379        46 hours ago        9.19MB
    alpine              latest              f70734b6a266        4 weeks ago         5.61MB

Використовуйте `docker rmi`, щоб видалити образ. Це також видаляє всі непотрібні проміжні образи, на яких він побудований. Якщо ви не видаляєте образи одразу, вони можуть накопичуватися з часом.
